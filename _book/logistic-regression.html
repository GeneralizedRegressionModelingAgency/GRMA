<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Logistic Regression | An Introduction to Generalized Linear Models</title>
  <meta name="description" content="Generalized Linear Models for non-statistics graduate students" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Logistic Regression | An Introduction to Generalized Linear Models" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Generalized Linear Models for non-statistics graduate students" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Logistic Regression | An Introduction to Generalized Linear Models" />
  
  <meta name="twitter:description" content="Generalized Linear Models for non-statistics graduate students" />
  

<meta name="author" content="Emma Grossman, Leah Marcus, Emily Palmer, Katherine Pulham, Andrew Rumments" />


<meta name="date" content="2021-03-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear.html"/>
<link rel="next" href="poisson-regression.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">GRMs</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About this book</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#linear-models---a-kind-of-generalized-linear-model"><i class="fa fa-check"></i><b>2.1</b> Linear models - a kind of Generalized Linear Model</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#linear-assumptions"><i class="fa fa-check"></i><b>2.2</b> Assumptions of Linear Models</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-happens-when-we-break-the-assumptions-of-linear-models"><i class="fa fa-check"></i><b>2.3</b> What happens when we break the assumptions of linear models</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#parameter-estimation"><i class="fa fa-check"></i><b>2.4</b> Parameter estimation</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#lms-and-glms-in-r"><i class="fa fa-check"></i><b>2.5</b> LMs and GLMs in R</a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#definitions"><i class="fa fa-check"></i><b>2.6</b> Some definitions</a></li>
<li class="chapter" data-level="2.7" data-path="intro.html"><a href="intro.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
<li class="chapter" data-level="2.8" data-path="intro.html"><a href="intro.html#examples"><i class="fa fa-check"></i><b>2.8</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html"><i class="fa fa-check"></i><b>3</b> How are GLMs “different”?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#introdution"><i class="fa fa-check"></i><b>3.1</b> Introdution</a></li>
<li class="chapter" data-level="3.2" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#assumptions-of-a-glm"><i class="fa fa-check"></i><b>3.2</b> Assumptions of a GLM</a></li>
<li class="chapter" data-level="3.3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#framework"><i class="fa fa-check"></i><b>3.3</b> Framework</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#exponential-dispersion-models"><i class="fa fa-check"></i><b>3.3.1</b> Exponential Dispersion models</a></li>
<li class="chapter" data-level="3.3.2" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#properties-of-edms"><i class="fa fa-check"></i><b>3.3.2</b> Properties of EDMs</a></li>
<li class="chapter" data-level="3.3.3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#linking-the-edm-to-the-explanatory-data"><i class="fa fa-check"></i><b>3.3.3</b> Linking the EDM to the explanatory data</a></li>
<li class="chapter" data-level="3.3.4" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#formal-definition-of-a-glm"><i class="fa fa-check"></i><b>3.3.4</b> Formal definition of a GLM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>4</b> Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear.html"><a href="linear.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="linear.html"><a href="linear.html#a-good-example-of-simple-linear-regression"><i class="fa fa-check"></i><b>4.2</b> A “Good” Example of Simple Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear.html"><a href="linear.html#a-bad-example-of-simple-linear-regression"><i class="fa fa-check"></i><b>4.3</b> A “Bad” Example of Simple Linear Regression</a></li>
<li class="chapter" data-level="4.4" data-path="linear.html"><a href="linear.html#summary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#what-is-binomial-data"><i class="fa fa-check"></i><b>5.1</b> What is Binomial Data?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#refresher-bernoulli-and-binomial"><i class="fa fa-check"></i><b>5.1.1</b> Refresher: Bernoulli and Binomial</a></li>
<li class="chapter" data-level="5.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#representing-the-bernoulli-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Representing the Bernoulli distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#representing-the-binomial-distribution"><i class="fa fa-check"></i><b>5.1.3</b> Representing the binomial distribution</a></li>
<li class="chapter" data-level="5.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#what-does-a-bunch-of-binomial-data-look-like-then"><i class="fa fa-check"></i><b>5.1.4</b> What does a bunch of binomial data look like then?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#why-doesnt-ols-work-for-binomial-data"><i class="fa fa-check"></i><b>5.2</b> Why doesn’t OLS work for Binomial Data?</a></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#link-functions-we-can-use"><i class="fa fa-check"></i><b>5.3</b> Link functions we can use</a></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#ed50"><i class="fa fa-check"></i><b>5.4</b> ED50</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i><b>6</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="poisson-regression.html"><a href="poisson-regression.html#what-is-poisson-data"><i class="fa fa-check"></i><b>6.1</b> What is Poisson data?</a></li>
<li class="chapter" data-level="6.2" data-path="poisson-regression.html"><a href="poisson-regression.html#why-ordinary-least-squares-does-not-work-for-poisson-data"><i class="fa fa-check"></i><b>6.2</b> Why ordinary least squares does not work for Poisson data</a></li>
<li class="chapter" data-level="6.3" data-path="poisson-regression.html"><a href="poisson-regression.html#link-functions-for-poisson-glms"><i class="fa fa-check"></i><b>6.3</b> Link functions for Poisson GLM’s</a></li>
<li class="chapter" data-level="6.4" data-path="poisson-regression.html"><a href="poisson-regression.html#problems-of-overdispersion-and-solutions"><i class="fa fa-check"></i><b>6.4</b> Problems of overdispersion and solutions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Generalized Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Logistic Regression</h1>
<p>We’ve asked you thus far to take our word regarding Generalized Linear Models (GLMs). In this chapter, we’re going to take a look at a certain type of data that we know violates our assumptions: binomial data. Here we’ll examine binomial data, see why ordinary least sqares falls apart, and consider some alternative methods (spoiler: it’s going to be one of our generalized linear models).</p>
<div id="what-is-binomial-data" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> What is Binomial Data?</h2>
<div id="refresher-bernoulli-and-binomial" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Refresher: Bernoulli and Binomial</h3>
<p>Bernoulli and Binomial random variables are some of the most important to consider, because they get at real the roots of representing the stata of the world. A Bernoulli random variable considers the simplest possible datum: whether something is, or it isn’t. Numerically, we represent this as 1 or 0, just like computer data. And while I can use this to measure yes/no type data, such as “does a person own at least one pet?”, an astute observer will realise the same idea can apply to a variety concepts including:</p>
<ul>
<li>success / failure,</li>
<li>wins / losses,</li>
<li>heads / tails,</li>
<li>exists / doesn’t exist</li>
<li>has / doesn’t have</li>
<li>is a member of a group of interest / is not a member</li>
<li>or even, was my prediction realized?</li>
</ul>
<p>Often, whatever resulted in creating this single Bernoulli instance might be something that repeats. In fact, if we hope to apply statistics to it, there needs to be many instances. If we can assume a constant probability across all iterations of the events we’re interested in, but want to ask questions about them in aggregate, that’s Bernoulli Binomial (or just binomial for short). Some examples of questions that fall into binomial data would be:</p>
<ul>
<li>is this strategy effective (i.e. does it affect the rates of success or failure across many trials?)</li>
<li>is this baseball team better than another (i.e. do they win more games, not just a game?)</li>
<li>if I bet money on getting four heads in a row, what kind of odds makes that a good gamble?</li>
<li>if a mailcarrier is moving to a new route, if I know how many homes are <span class="math inline">\(\mu\)</span>-probable to have an angry dog, what’s the liklihood that mail carrier will encounter an angry dog?</li>
<li>if <span class="math inline">\(\mu\)</span>-proportion of students have a medical condition I’m interested in, how many students will I need to talk to in order for it to be more likely than not I’ll talk to x of them? Way more likely than not? Almost certain?</li>
<li>if my predictions are p-percent accurate, and the cost of failure is m dollars, is it worth spending more money for more accurate predictions?</li>
</ul>
<p>Hopefully you’ll agree with me that questions of this type are actually extremely common.</p>
</div>
<div id="representing-the-bernoulli-distribution" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Representing the Bernoulli distribution</h3>
<p>Different texts use different language, so it’s to explain how we’ll be talking about Bernoulli and binomial distributions here. First, here’s the /Bernoulli distribution/ probability mass function.</p>
<p><span class="math display">\[\mathcal{P}(y;\mu)=\mu^{y}(1-\mu)^{1-\mu},\]</span>
where <span class="math inline">\(y\in\{0, 1\}, 0\leq\mu\leq1\)</span>.</p>
<p>This might be a bit different than what you’ve seen before, so lets talk about each variable.</p>
<ul>
<li><span class="math inline">\(\mathcal{P}\)</span>: (<span class="math inline">\(\texttt{\\mathcal\{P\}}\)</span> in TeX) is the probability mass function of a certain outcome. <span class="math inline">\(\mathcal{P}(y;\mu)\)</span> then is the probability of event <span class="math inline">\(y\)</span> when the success has a probability of <span class="math inline">\(\mu\)</span>.</li>
<li><span class="math inline">\(y\)</span>: is the number of successes. Since this is a single trial, it can only be 0 or 1 (described by the statement <span class="math inline">\(y\in\{0, 1\}\)</span>).
<ul>
<li>Basically, it lets us choose whether we’re predicting success or predicting failure all in one function.</li>
<li>If we’re interested in the liklihood of an event not occuring <span class="math inline">\(y=0\)</span>, <span class="math inline">\(\mu^0=1\)</span>, and the PMF simplifies to <span class="math inline">\(\mathcal{P(1,\mu)}=1-\mu\)</span>.</li>
<li>Conversely, if we’re interetsed in an event occuring, <span class="math inline">\(y-1\)</span>, <span class="math inline">\((1-\mu)^{(1-1)}=1\)</span>, and the PMF simplifies to <span class="math inline">\(\mathcal{P(1,\mu)}=\mu\)</span>.</li>
</ul></li>
<li><span class="math inline">\(\mu\)</span>: (<span class="math inline">\(\texttt{\\mu}\)</span> in TeX) is the probability of the event. But wait, isn’t <span class="math inline">\(\mu\)</span> the mean, and also the expected value? <strong>Yes, exactly!</strong> Those are all the same in this PMF, so we use the same variable. Because many text books need you to consider those ideas separatey so you can discover that relationship, you might see it recorded elsewhere as <span class="math inline">\(p\)</span>, but since demonstrating that is beyond our scope here, we’ll use <span class="math inline">\(\mu\)</span> to reduce complexity.</li>
<li>It complicates using the formula elsewhere, but we could rewrite this as:
<span class="math display">\[
\mathcal{P}(y;\mu)=
\begin{cases}
\mu&amp;\textrm{if }y=1\\
1-\mu&amp;\textrm{if }y=0\\
\end{cases}, 0\leq\mu\leq1
\]</span></li>
</ul>
</div>
<div id="representing-the-binomial-distribution" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Representing the binomial distribution</h3>
<p>Intuitively, you likely recall that if <span class="math inline">\(P(\textrm{heads on one coinflip})=\frac{1}{2}\)</span>, then <span class="math display">\[P(\textrm{heads twice on two coinflips})=P(\textrm{heads on one coinflip})^2=\frac{1}{2}\times\frac{1}{2}=\frac{1}{4}\]</span>. However, just like the above PMF accounts for yes and no, our binomial PMF <span class="math inline">\(\mathcal{P}\)</span> needs to account for yes and no, how many yesses we’re looking for, and how many trials there are overall. So here, we’ll use:</p>
<p><span class="math display">\[\mathcal{P}(y;\mu,m)=\binom{m}{my}\mu^y(1-\mu)^{m(1-y)}\]</span>
where <span class="math inline">\(y={0, 1/m, 2/m, ..., 1}, m \textrm{ is a positive integer, and }0&lt;\mu&lt;1.\)</span></p>
<p>Like before, this format might be a bit different than what you may have seen elsewhere, so let’s break it down.</p>
<ul>
<li>A reminder, <span class="math inline">\(\binom{n}{k}\)</span>, (written <code>\texttt{\binom{n}{k}}</code> in TeX) is a combination <span class="math inline">\(n\)</span> choose <span class="math inline">\(k\)</span>. More detail <a href="https://en.wikipedia.org/wiki/Combination">here.</a></li>
<li><span class="math inline">\(\mathcal{P}\)</span> (<span class="math inline">\(\texttt{\\mathcal\{P\}}\)</span> in TeX) is the probability mass function (PMF) aka probability function. <span class="math inline">\(\mathcal{P}(y;\mu,m)\)</span> then is the probability function of a binomial distribution for event <span class="math inline">\(y\)</span> in the case of <span class="math inline">\(m\)</span> Bernoulli trials with each trial having a probability of <span class="math inline">\(\mu\)</span>.</li>
<li><span class="math inline">\(m\)</span> here is simply the number of trials easy.</li>
<li><span class="math inline">\(y\)</span> here measures the <em>proportion</em> of successes (and actually it really is above as well–I’ll get to that in a moment).
<ul>
<li>For example, if there are 4 trials and we want to see two successes, <span class="math inline">\(y/m=2/4=0.5\)</span></li>
<li>This interacts interestingly in the combinatorial component. That <span class="math inline">\(my\)</span> will always equal the numerator of <span class="math inline">\(y\)</span>. In the previous example then, the <em>choose</em> part of <span class="math inline">\(m\)</span> choose <span class="math inline">\(my\)</span> (aka <span class="math inline">\(\binom{m}{my}\)</span>) will equal <span class="math inline">\(\binom{4}{2}\)</span>.</li>
</ul></li>
<li>If <span class="math inline">\(y=0\)</span>, <span class="math inline">\(\binom{m}{m0}=\binom{m}{0}=1\)</span> (any choose 0, like any number raised to 0 equals 1). So, looking at this formula, if I want to know the odds of not getting any heads on four coin flips, then
<ul>
<li><span class="math display">\[\mathcal{P}(y;\mu,m)=\binom{m}{my}\mu^y(1-\mu)^{m(1-y)}=\binom{4}{4\times0}\mu^0(1-\mu)^{4(1-0)}=1\times1\times(1-\mu)^4\]</span></li>
<li>So assuming a fair coin where <span class="math inline">\(\mu=\frac{1}{2}\)</span>, then <span class="math inline">\(\mathcal{P}(y;\mu,m)=\mu^4=(\frac{1}{2})^4=\frac{1}{16}\)</span>, which fits our expectations. Neat!</li>
</ul></li>
</ul>
</div>
<div id="what-does-a-bunch-of-binomial-data-look-like-then" class="section level3" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> What does a bunch of binomial data look like then?</h3>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="logistic-regression.html#cb24-1"></a>possibilities &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>) <span class="co"># either heads (1) or tails (0).</span></span>
<span id="cb24-2"><a href="logistic-regression.html#cb24-2"></a>                        <span class="co"># c(...) makes a list of whatever you include in the ...</span></span>
<span id="cb24-3"><a href="logistic-regression.html#cb24-3"></a>probabilities &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb24-4"><a href="logistic-regression.html#cb24-4"></a>                        <span class="co"># a 50% chance of heads, a 50% chance of tails.</span></span>
<span id="cb24-5"><a href="logistic-regression.html#cb24-5"></a>(<span class="dt">flip_data =</span> <span class="kw">sample</span>(<span class="dt">x =</span> possibilities, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>, <span class="dt">prob=</span>probabilities))</span></code></pre></div>
<pre><code>##  [1] 1 0 0 1 0 0 1 0 0 1</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="logistic-regression.html#cb26-1"></a>                        <span class="co"># x is the vector of possible outcomes.</span></span>
<span id="cb26-2"><a href="logistic-regression.html#cb26-2"></a>                        <span class="co"># size is the number of coin flips I want.</span></span>
<span id="cb26-3"><a href="logistic-regression.html#cb26-3"></a>                        <span class="co"># we are flipping coin, not drawing from a deck, so ne</span></span>
<span id="cb26-4"><a href="logistic-regression.html#cb26-4"></a>                        <span class="co">#   second flip is just as likely to get heads or tails</span></span>
<span id="cb26-5"><a href="logistic-regression.html#cb26-5"></a>                        <span class="co">#   as the first: we *do* replace.</span></span>
<span id="cb26-6"><a href="logistic-regression.html#cb26-6"></a>                        <span class="co"># prob is a vector of probabilities for each x</span></span>
<span id="cb26-7"><a href="logistic-regression.html#cb26-7"></a>                        <span class="co"># encasing the whole thing in parentheses shows the</span></span>
<span id="cb26-8"><a href="logistic-regression.html#cb26-8"></a>                        <span class="co">#   value of the variable you set as you set it</span></span></code></pre></div>
<p>So zeros and ones, we expect that (sorry if you’re the 2 in <span class="math inline">\(2^{10}\)</span> readers who see all zeroes or ones). Lets check a simple histogram.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="logistic-regression.html#cb27-1"></a><span class="kw">hist</span>(flip_data)         <span class="co"># hist() generates a simple historam. We could make</span></span></code></pre></div>
<p><img src="Bookdown_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="logistic-regression.html#cb28-1"></a>                        <span class="co"># this prettier with ggplot2, but we don&#39;t need that right</span></span>
<span id="cb28-2"><a href="logistic-regression.html#cb28-2"></a>                        <span class="co"># now.</span></span></code></pre></div>
</div>
</div>
<div id="why-doesnt-ols-work-for-binomial-data" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Why doesn’t OLS work for Binomial Data?</h2>
</div>
<div id="link-functions-we-can-use" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Link functions we can use</h2>
</div>
<div id="ed50" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> ED50</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="poisson-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Bookdown.pdf", "Bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
