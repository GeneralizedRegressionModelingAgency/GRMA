<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Linear Models | An Introduction to Generalized Linear Models</title>
  <meta name="description" content="Generalized Linear Models for non-statistics graduate students" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Linear Models | An Introduction to Generalized Linear Models" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Generalized Linear Models for non-statistics graduate students" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Linear Models | An Introduction to Generalized Linear Models" />
  
  <meta name="twitter:description" content="Generalized Linear Models for non-statistics graduate students" />
  

<meta name="author" content="Emma Grossman, Leah Marcus, Emily Palmer, Katherine Pulham, Andrew Rumments" />


<meta name="date" content="2021-03-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="how-are-glms-different.html"/>
<link rel="next" href="logistic-regression.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">GRMs</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About this book</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#linear-models---a-kind-of-generalized-linear-model"><i class="fa fa-check"></i><b>2.1</b> Linear models - a kind of Generalized Linear Model</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#linear-assumptions"><i class="fa fa-check"></i><b>2.2</b> Assumptions of Linear Models</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-happens-when-we-break-the-assumptions-of-linear-models"><i class="fa fa-check"></i><b>2.3</b> What happens when we break the assumptions of linear models</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#parameter-estimation"><i class="fa fa-check"></i><b>2.4</b> Parameter estimation</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#lms-and-glms-in-r"><i class="fa fa-check"></i><b>2.5</b> LMs and GLMs in R</a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#definitions"><i class="fa fa-check"></i><b>2.6</b> Some definitions</a></li>
<li class="chapter" data-level="2.7" data-path="intro.html"><a href="intro.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
<li class="chapter" data-level="2.8" data-path="intro.html"><a href="intro.html#examples"><i class="fa fa-check"></i><b>2.8</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html"><i class="fa fa-check"></i><b>3</b> How are GLMs “different”?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#introdution"><i class="fa fa-check"></i><b>3.1</b> Introdution</a></li>
<li class="chapter" data-level="3.2" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#assumptions-of-a-glm"><i class="fa fa-check"></i><b>3.2</b> Assumptions of a GLM</a></li>
<li class="chapter" data-level="3.3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#framework"><i class="fa fa-check"></i><b>3.3</b> Framework</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#exponential-dispersion-models"><i class="fa fa-check"></i><b>3.3.1</b> Exponential Dispersion models</a></li>
<li class="chapter" data-level="3.3.2" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#properties-of-edms"><i class="fa fa-check"></i><b>3.3.2</b> Properties of EDMs</a></li>
<li class="chapter" data-level="3.3.3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#linking-the-edm-to-the-explanatory-data"><i class="fa fa-check"></i><b>3.3.3</b> Linking the EDM to the explanatory data</a></li>
<li class="chapter" data-level="3.3.4" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#formal-definition-of-a-glm"><i class="fa fa-check"></i><b>3.3.4</b> Formal definition of a GLM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>4</b> Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear.html"><a href="linear.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="linear.html"><a href="linear.html#a-good-example-of-simple-linear-regression"><i class="fa fa-check"></i><b>4.2</b> A “Good” Example of Simple Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear.html"><a href="linear.html#a-bad-example-of-simple-linear-regression"><i class="fa fa-check"></i><b>4.3</b> A “Bad” Example of Simple Linear Regression</a></li>
<li class="chapter" data-level="4.4" data-path="linear.html"><a href="linear.html#summary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#what-is-binomial-data"><i class="fa fa-check"></i><b>5.1</b> What is Binomial Data?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#refresher-bernoulli-and-binomial"><i class="fa fa-check"></i><b>5.1.1</b> Refresher: Bernoulli and Binomial</a></li>
<li class="chapter" data-level="5.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#representing-the-bernoulli-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Representing the Bernoulli distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#representing-the-binomial-distribution"><i class="fa fa-check"></i><b>5.1.3</b> Representing the binomial distribution</a></li>
<li class="chapter" data-level="5.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#what-does-a-bunch-of-binomial-data-look-like-then"><i class="fa fa-check"></i><b>5.1.4</b> What does a bunch of binomial data look like then?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#why-doesnt-ols-work-for-binomial-data"><i class="fa fa-check"></i><b>5.2</b> Why doesn’t OLS work for Binomial Data?</a></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#link-functions-we-can-use"><i class="fa fa-check"></i><b>5.3</b> Link functions we can use</a></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#ed50"><i class="fa fa-check"></i><b>5.4</b> ED50</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i><b>6</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="poisson-regression.html"><a href="poisson-regression.html#what-is-poisson-data"><i class="fa fa-check"></i><b>6.1</b> What is Poisson data?</a></li>
<li class="chapter" data-level="6.2" data-path="poisson-regression.html"><a href="poisson-regression.html#why-ordinary-least-squares-does-not-work-for-poisson-data"><i class="fa fa-check"></i><b>6.2</b> Why ordinary least squares does not work for Poisson data</a></li>
<li class="chapter" data-level="6.3" data-path="poisson-regression.html"><a href="poisson-regression.html#link-functions-for-poisson-glms"><i class="fa fa-check"></i><b>6.3</b> Link functions for Poisson GLM’s</a></li>
<li class="chapter" data-level="6.4" data-path="poisson-regression.html"><a href="poisson-regression.html#problems-of-overdispersion-and-solutions"><i class="fa fa-check"></i><b>6.4</b> Problems of overdispersion and solutions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Generalized Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Linear Models</h1>
<div id="introduction" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<p>At this point, you are likely familiar with linear regression. As discussed before, linear regression models are a special case of generalized regression model that we use when the data are normally distributed and have constant variance. We can think of linear regression models in the same terms we think of other regression models.</p>
<p>The two components of a regression model are the random component and the systematic component and for linear regression,</p>
<p><span class="math display">\[
\begin{cases}
  \text{var}[y_i] = \sigma^2/w_i \\
  \mu_i = \beta_0 + \sum_{j=1}^{p}\beta_jx_{ji}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(w_i\)</span> are prior weights and <span class="math inline">\(w_i\)</span> and <span class="math inline">\(\text{E}[y_i] = \mu_i\)</span> are known.</p>
<p>When our linear regression has two <span class="math inline">\(\beta_j\)</span> coefficients and the systematic compnent looks like <span class="math inline">\(\mu = \beta_0 + \beta_1x_1\)</span>, it is called <em>simple linear regression</em>. If we have more than two <span class="math inline">\(\beta_j\)</span> coefficients, our regression model is called <em>multiple linear regression model</em> or <em>multiple regression model</em>.</p>
<p>When all prior weights <span class="math inline">\(w_i\)</span> are equal to one, our regression model is refered to as <em>ordinary linear regression model</em> as opposed to when our prior weights <span class="math inline">\(w_i\)</span> have values other than one and is called a <em>weighted linear regression model</em>.</p>
<p>As mentioned before, the assumptions belonging to linear regression are:</p>
<ol style="list-style-type: decimal">
<li>The relationship between <span class="math inline">\(\mu\)</span> and each explanatory variable is <strong>linear</strong>.</li>
<li>The unexplained variation in our response is constant, otherwise known as <strong>constant variance</strong>.</li>
<li>Each datam is <strong>independent</strong> of all other data points.</li>
</ol>
</div>
<div id="a-good-example-of-simple-linear-regression" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> A “Good” Example of Simple Linear Regression</h2>
<p><code>trees</code> is a data set that comes with R. It has three variables, the diameter (mistakenly named girth in the data set), height and volume of each of the thirty-one trees in the data set. If we just type <code>data(trees)</code> into the Console, R will retreive it for us. This is the data set I’m going to use to showcase a good example of simple linear regression. At this time, let’s also make sure we have the <code>tidyverse</code> (which includes <code>ggplot</code> for graphing) loaded.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="linear.html#cb7-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb7-2"><a href="linear.html#cb7-2"></a><span class="kw">data</span>(trees)</span>
<span id="cb7-3"><a href="linear.html#cb7-3"></a></span>
<span id="cb7-4"><a href="linear.html#cb7-4"></a><span class="co"># Let&#39;s change the name from Girth to Diameter:</span></span>
<span id="cb7-5"><a href="linear.html#cb7-5"></a>trees &lt;-<span class="st"> </span>trees <span class="op">%&gt;%</span></span>
<span id="cb7-6"><a href="linear.html#cb7-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Diameter =</span> Girth) <span class="op">%&gt;%</span></span>
<span id="cb7-7"><a href="linear.html#cb7-7"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>Girth)</span>
<span id="cb7-8"><a href="linear.html#cb7-8"></a></span>
<span id="cb7-9"><a href="linear.html#cb7-9"></a><span class="kw">head</span>(trees)</span></code></pre></div>
<pre><code>##   Height Volume Diameter
## 1     70   10.3      8.3
## 2     65   10.3      8.6
## 3     63   10.2      8.8
## 4     72   16.4     10.5
## 5     81   18.8     10.7
## 6     83   19.7     10.8</code></pre>
<p>Let’s look at the scatterplot between these two variables, Diameter and Height.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="linear.html#cb9-1"></a><span class="kw">ggplot</span>(trees)<span class="op">+</span></span>
<span id="cb9-2"><a href="linear.html#cb9-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Diameter, <span class="dt">y =</span>  Height))<span class="op">+</span></span>
<span id="cb9-3"><a href="linear.html#cb9-3"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Diameter (inches)&quot;</span>)<span class="op">+</span></span>
<span id="cb9-4"><a href="linear.html#cb9-4"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Height (feet)&quot;</span>)<span class="op">+</span></span>
<span id="cb9-5"><a href="linear.html#cb9-5"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Diameter and Height of 31 Black Cherry Trees&quot;</span>)</span></code></pre></div>
<p><img src="Bookdown_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>When plotting the explanatory variable (Diameter) against the response variable (Height), we are looking for linearity, since that is one of our conditions of fitting a linear model to our data. Though the relationship between these two variables seems to be moderate or moderately-weak, it does indeed look linear.</p>
<p>The next condition is constant variance, which we evaluate by looking at the residuals. We first need to produce those residuals which we do by fitting the model. The <code>lm</code> function automatically saves residuals, among other information, about the model:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="linear.html#cb10-1"></a>mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Height <span class="op">~</span><span class="st"> </span>Diameter, <span class="dt">data =</span> trees)</span>
<span id="cb10-2"><a href="linear.html#cb10-2"></a><span class="kw">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Height ~ Diameter, data = trees)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -12.5816  -2.7686   0.3163   2.4728   9.9456 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  62.0313     4.3833  14.152 1.49e-14 ***
## Diameter      1.0544     0.3222   3.272  0.00276 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.538 on 29 degrees of freedom
## Multiple R-squared:  0.2697, Adjusted R-squared:  0.2445 
## F-statistic: 10.71 on 1 and 29 DF,  p-value: 0.002758</code></pre>
<p>We can view those residuals by typing <code>head(mod$residuals)</code>, let’s just look at the first six to get an idea of them.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="linear.html#cb12-1"></a><span class="kw">head</span>(mod<span class="op">$</span>residuals)</span></code></pre></div>
<pre><code>##         1         2         3         4         5         6 
## -0.782575 -6.098886 -8.309759 -1.102186  7.686940  9.581503</code></pre>
<p>Next, we should view the actual residuals to check for normality and constant variance.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="linear.html#cb14-1"></a><span class="kw">ggplot</span>()<span class="op">+</span></span>
<span id="cb14-2"><a href="linear.html#cb14-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> trees<span class="op">$</span>Diameter, <span class="dt">y =</span> mod<span class="op">$</span>residuals))<span class="op">+</span></span>
<span id="cb14-3"><a href="linear.html#cb14-3"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>)<span class="op">+</span></span>
<span id="cb14-4"><a href="linear.html#cb14-4"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Diameter (inches)&quot;</span>)<span class="op">+</span></span>
<span id="cb14-5"><a href="linear.html#cb14-5"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;residuals&quot;</span>)<span class="op">+</span></span>
<span id="cb14-6"><a href="linear.html#cb14-6"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Residual plot&quot;</span>)</span></code></pre></div>
<p><img src="Bookdown_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>When examining the residual plot, we are looking for two things: randomness and constant variance. Both are indications that our residuals are <span class="math inline">\(N(0,\sigma^2)\)</span>. If we don’t see randomness, i.e. there is a pattern, that indicates that are data are not linear. Constant variance is a necessary condition because when we assume the residuals are <span class="math inline">\(N(0,\sigma^2)\)</span>, the variance parameter is one estimate, <span class="math inline">\(\sigma^2\)</span>. If we see the variance of the residuals changing, then that indicates that the one variance parameter isn’t sufficient and since a normal distribution only has one variance parameter, it might not be the best model for the data.</p>
<p>There does not seem to be a pattern to the residuals, so they look random. This again suggests our data are linear. As for constant variance, however, it seems have more variation in the middle than we do at the ends of our graph. It isn’t too bad though and we can proceed.</p>
<p>Our last condition is independence. We evaluate independence by looking at how the data were collected. We can find out more information about the trees data set by typing <code>?trees</code> into the Console, which will bring up the help file on the data set. This data comes from 31 black cherry trees, felled for timber. Though we don’t have much information, the trees could be independent if they were randomly selected to be measured.</p>
<p>The <code>trees</code> data set isn’t perfect for linear regression, but it meets most of our assumptions. I’ll now move on to an example of linear regression with a data set that is not linear.</p>
</div>
<div id="a-bad-example-of-simple-linear-regression" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> A “Bad” Example of Simple Linear Regression</h2>
<p>First, we load in the <code>mtcars</code> data set, look at the first six obervations of each variable.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="linear.html#cb15-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb15-2"><a href="linear.html#cb15-2"></a><span class="kw">data</span>(mtcars)</span>
<span id="cb15-3"><a href="linear.html#cb15-3"></a><span class="kw">head</span>(mtcars)</span></code></pre></div>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
<p>Now, let’s take a look at a scatterplot of two variables in <code>mtcars</code>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="linear.html#cb17-1"></a><span class="kw">ggplot</span>(mtcars)<span class="op">+</span></span>
<span id="cb17-2"><a href="linear.html#cb17-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> wt, <span class="dt">y =</span> mpg))<span class="op">+</span></span>
<span id="cb17-3"><a href="linear.html#cb17-3"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Weight (in thousands of lbs)&quot;</span>)<span class="op">+</span></span>
<span id="cb17-4"><a href="linear.html#cb17-4"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Miles per Gallon&quot;</span>)<span class="op">+</span></span>
<span id="cb17-5"><a href="linear.html#cb17-5"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Car&#39;s Mileage based on Weight&quot;</span>)</span></code></pre></div>
<p><img src="Bookdown_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>We can see that this graph looks mostly linear, there seems to be a strong, negative relationship between a car’s weight and and its mpg. There is some nonlinearity because of the three points for cars with a weight above 5,000 lbs; the graph look a bit curved so we should be cautious as we proceed.</p>
<p>If we add a smoothed line to the plot, we can see that it is indeed curved.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="linear.html#cb18-1"></a><span class="kw">ggplot</span>(mtcars)<span class="op">+</span></span>
<span id="cb18-2"><a href="linear.html#cb18-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> wt, <span class="dt">y =</span> mpg))<span class="op">+</span></span>
<span id="cb18-3"><a href="linear.html#cb18-3"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">x =</span> wt, <span class="dt">y =</span> mpg), <span class="dt">se =</span> <span class="ot">FALSE</span>)<span class="op">+</span></span>
<span id="cb18-4"><a href="linear.html#cb18-4"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Weight (in thousands of lbs)&quot;</span>)<span class="op">+</span></span>
<span id="cb18-5"><a href="linear.html#cb18-5"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Miles per Gallon&quot;</span>)<span class="op">+</span></span>
<span id="cb18-6"><a href="linear.html#cb18-6"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Car&#39;s Mileage based on Weight&quot;</span>)</span></code></pre></div>
<p><img src="Bookdown_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>This graph addresses one of the assumptions of linear regression: linearity. Next, let’s check whether there is constant variation. We do this with a residual plot. First, we have to fit a model to our data in order to obtain residuals.</p>
<p>To fit a model, we use the <code>lm</code> function.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="linear.html#cb19-1"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>wt, <span class="dt">data =</span> mtcars)</span>
<span id="cb19-2"><a href="linear.html#cb19-2"></a><span class="kw">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5432 -2.3647 -0.1252  1.4096  6.8727 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***
## wt           -5.3445     0.5591  -9.559 1.29e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.046 on 30 degrees of freedom
## Multiple R-squared:  0.7528, Adjusted R-squared:  0.7446 
## F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10</code></pre>
<p>Among other things, the <code>lm</code> function saves the residuals.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="linear.html#cb21-1"></a><span class="kw">head</span>(fit<span class="op">$</span>residuals)</span></code></pre></div>
<pre><code>##         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive 
##        -2.2826106        -0.9197704        -2.0859521         1.2973499 
## Hornet Sportabout           Valiant 
##        -0.2001440        -0.6932545</code></pre>
<p>We can plot the residuals against the weight and look for randomness and constant variance.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="linear.html#cb23-1"></a><span class="kw">ggplot</span>()<span class="op">+</span></span>
<span id="cb23-2"><a href="linear.html#cb23-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> mtcars<span class="op">$</span>wt, <span class="dt">y =</span> fit<span class="op">$</span>residuals))<span class="op">+</span></span>
<span id="cb23-3"><a href="linear.html#cb23-3"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>)<span class="op">+</span></span>
<span id="cb23-4"><a href="linear.html#cb23-4"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Weight (in thousands of lbs)&quot;</span>)<span class="op">+</span></span>
<span id="cb23-5"><a href="linear.html#cb23-5"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;residuals&quot;</span>)<span class="op">+</span></span>
<span id="cb23-6"><a href="linear.html#cb23-6"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Residual plot&quot;</span>)</span></code></pre></div>
<p><img src="Bookdown_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The graph above seems to have a curved pattern. The cars that weigh the least have large residuals, all above the zero line. The residuals of the cars that weigh between 2,250 and 4,250 lbs are lower and seem to be centered on the zero line. The cars that weigh the heaviest also have large residuals. So, there seems to be a parabola shape to our data, indicating nonlinearity.</p>
<p>As for constant variance, we could go either way. As we look at different car weights, the distance between the top-most point and the bottom-most point is fairly similar indicating we could have constant variance. That might be enough evidence for some folks, but others might point out that the though the cars that weigh the least and the cars that weigh between 2,250 and 4,250 lbs are similar, cars that weigh the most have very small variance comparatively.</p>
<p>Lastly, we need independence. We evaluate independence by checking the data and how it was obtained. If you type <code>?mtcars</code> into the Console, the Help file with pop up for the <code>mtcars</code> data set. Here, we learn that the data came from a 1981 textbook on biometrics. While there isn’t much information on the data here, we can look to the data for answers. The data records many different attributes of certain make and models of cars. In general, one car of a particular make and model is unlikely to influence the attributes of another car. The only exception I can think of is cars of the same make. Perhaps two cars made by Mazda might not be totally independent, but this would be a more serious concern if we had observations of two cars with the same make and model but different years. So, this last condition is likely met.</p>
<p>In general, using <code>wt</code> to explain <code>mpg</code> in the <code>mtcars</code> data set could have gone worse. The violations of the assumptions were quite small and some folks might find that thee transgressions to be ignorable. We should not take violations lightly, however, especially since there are steps we can take if our data isn’t meeting the assumptions. Transformations are always an option but if we have data of a particular kind, e.g. binary data or count data, we can use generalized linear models instead.</p>
</div>
<div id="summary" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Summary</h2>
<p>Often we want to fit our data with a model to better understand our response variable or to try to predict new events. The ideal case (because it is common, widely used and easy to interpret) is that we can fit a linear model to the data. In order to do so, the relationship between the response variable and the explanatory variable(s) needs to be linear, we need to have constant variance, and the data needs to be independent.</p>
<p>If these condtions are not met, then there are steps we can take. One of which is transformations, we will not be going over that. The other is to fit a generalized linear model, which doesn’t necessarily assume the data come from a normal distribution.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="how-are-glms-different.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Bookdown.pdf", "Bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
