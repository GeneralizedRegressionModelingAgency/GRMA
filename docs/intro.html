<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction | An Introduction to Generalized Linear Models</title>
  <meta name="description" content="Generalized Linear Models for non-statistics graduate students" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction | An Introduction to Generalized Linear Models" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Generalized Linear Models for non-statistics graduate students" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction | An Introduction to Generalized Linear Models" />
  
  <meta name="twitter:description" content="Generalized Linear Models for non-statistics graduate students" />
  

<meta name="author" content="Emma Grossman, Leah Marcus, Emily Palmer, Katherine Pulham, Andrew Rumments" />


<meta name="date" content="2021-03-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="how-are-glms-different.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">GRMs</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About this book</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#linear-models---a-kind-of-generalized-linear-model"><i class="fa fa-check"></i><b>2.1</b> Linear models - a kind of Generalized Linear Model</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#linear-assumptions"><i class="fa fa-check"></i><b>2.2</b> Assumptions of Linear Models</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#reframing-linear-model"><i class="fa fa-check"></i><b>2.3</b> Reframing Linear Models as Generalized Linear Models</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#what-happens-when-we-break-the-assumptions-of-linear-models"><i class="fa fa-check"></i><b>2.4</b> What happens when we break the assumptions of linear models</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#parameter-estimation"><i class="fa fa-check"></i><b>2.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#lms-and-glms-in-r"><i class="fa fa-check"></i><b>2.6</b> LMs and GLMs in R</a></li>
<li class="chapter" data-level="2.7" data-path="intro.html"><a href="intro.html#definitions"><i class="fa fa-check"></i><b>2.7</b> Some definitions</a></li>
<li class="chapter" data-level="2.8" data-path="intro.html"><a href="intro.html#conclusion"><i class="fa fa-check"></i><b>2.8</b> Conclusion</a></li>
<li class="chapter" data-level="2.9" data-path="intro.html"><a href="intro.html#examples"><i class="fa fa-check"></i><b>2.9</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html"><i class="fa fa-check"></i><b>3</b> How are GLMs “different”?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#introdution"><i class="fa fa-check"></i><b>3.1</b> Introdution</a></li>
<li class="chapter" data-level="3.2" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#assumptions-of-a-glm"><i class="fa fa-check"></i><b>3.2</b> Assumptions of a GLM</a></li>
<li class="chapter" data-level="3.3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#framework"><i class="fa fa-check"></i><b>3.3</b> Framework</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#exponential-dispersion-models"><i class="fa fa-check"></i><b>3.3.1</b> Exponential Dispersion models</a></li>
<li class="chapter" data-level="3.3.2" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#properties-of-edms"><i class="fa fa-check"></i><b>3.3.2</b> Properties of EDMs</a></li>
<li class="chapter" data-level="3.3.3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#linking-the-edm-to-the-explanatory-data"><i class="fa fa-check"></i><b>3.3.3</b> Linking the EDM to the explanatory data</a></li>
<li class="chapter" data-level="3.3.4" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#formal-definition-of-a-glm"><i class="fa fa-check"></i><b>3.3.4</b> Formal definition of a GLM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>4</b> Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear.html"><a href="linear.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="linear.html"><a href="linear.html#a-good-example-of-simple-linear-regression"><i class="fa fa-check"></i><b>4.2</b> A “Good” Example of Simple Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear.html"><a href="linear.html#a-bad-example-of-simple-linear-regression"><i class="fa fa-check"></i><b>4.3</b> A “Bad” Example of Simple Linear Regression</a></li>
<li class="chapter" data-level="4.4" data-path="linear.html"><a href="linear.html#summary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#what-is-binomial-data"><i class="fa fa-check"></i><b>5.1</b> What is Binomial Data?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#refresher-bernoulli-and-binomial"><i class="fa fa-check"></i><b>5.1.1</b> Refresher: Bernoulli and Binomial</a></li>
<li class="chapter" data-level="5.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#representing-the-bernoulli-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Representing the Bernoulli distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#representing-the-binomial-distribution"><i class="fa fa-check"></i><b>5.1.3</b> Representing the binomial distribution</a></li>
<li class="chapter" data-level="5.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#what-does-a-bunch-of-binomial-data-look-like-then"><i class="fa fa-check"></i><b>5.1.4</b> What does a bunch of binomial data look like then?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#why-doesnt-ols-work-for-binomial-data"><i class="fa fa-check"></i><b>5.2</b> Why doesn’t OLS work for Binomial Data?</a></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#link-functions-we-can-use"><i class="fa fa-check"></i><b>5.3</b> Link functions we can use</a></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#ed50"><i class="fa fa-check"></i><b>5.4</b> ED50</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i><b>6</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="poisson-regression.html"><a href="poisson-regression.html#what-is-poisson-data"><i class="fa fa-check"></i><b>6.1</b> What is Poisson data?</a></li>
<li class="chapter" data-level="6.2" data-path="poisson-regression.html"><a href="poisson-regression.html#why-ordinary-least-squares-does-not-work-for-poisson-data"><i class="fa fa-check"></i><b>6.2</b> Why ordinary least squares does not work for Poisson data</a></li>
<li class="chapter" data-level="6.3" data-path="poisson-regression.html"><a href="poisson-regression.html#link-functions-for-poisson-glms"><i class="fa fa-check"></i><b>6.3</b> Link functions for Poisson GLM’s</a></li>
<li class="chapter" data-level="6.4" data-path="poisson-regression.html"><a href="poisson-regression.html#poisson-example"><i class="fa fa-check"></i><b>6.4</b> Poisson Example</a></li>
<li class="chapter" data-level="6.5" data-path="poisson-regression.html"><a href="poisson-regression.html#problems-of-overdispersion-and-solutions"><i class="fa fa-check"></i><b>6.5</b> Problems of overdispersion and solutions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Generalized Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction</h1>
<div id="linear-models---a-kind-of-generalized-linear-model" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Linear models - a kind of Generalized Linear Model</h2>
<p>If you have fit a linear model before, congratulations! You have already fit (one case) of a generalized linear model (GLM). Over the course of this book, we will explore the framework of Generalized Linear Models, why the Linear Model is a special case of a GLM, and two common type of GLMs, including logistic regression (for binary/binomial count data) and poisson regression (for count data).</p>
<p>Let’s review why we would fit a linear model. For linear regression, given our data, (if we make some key assumptions <a href="intro.html#linear-assumptions">2.2</a> ), we can perform inference or prediction by assuming that our response value forms a linear relationship with our explanatory variable (or variables). Generalized linear models expand upon the linear model assumptions to increase the kinds and ways we can use data to answer questions.</p>
<p>The reasoning behind linear models can be intuitive; if we have two continuous variables, and make a scatterplot our data, and see this:</p>
<div class="figure"><span id="fig:linearscatterplot"></span>
<img src="Bookdown_files/figure-html/linearscatterplot-1.png" alt="A scatterplot of points for two numeric variables." width="672" />
<p class="caption">
Figure 2.1: A scatterplot of points for two numeric variables.
</p>
</div>
<p>We might want to fit a straight line through the cloud of points, i.e. modeling the relationship linearly.</p>
<p><img src="images/put_a_bird.png" style="width:50.0%" /></p>
<div class="figure"><span id="fig:linearline"></span>
<img src="Bookdown_files/figure-html/linearline-1.png" alt="A scatterplot of points now with a _line of best fit_ added." width="672" />
<p class="caption">
Figure 2.2: A scatterplot of points now with a <em>line of best fit</em> added.
</p>
</div>
<p>To interpret this relationship and make predictions, we need to know the slope and intercept of this line. This is done by minimizing the least squares, which will be explored in Chapter 3 @ref{linear}. (will it? should it?)
<!-- ZZQ I think it might be a good idea to include a short section on how we fit linear models because it highlights how estimating parameters in GLMs is so different. -->
Linear models can also have multiple explanatory variables (<span class="math inline">\(X_1, \ldots X_p\)</span> instead of just one <span class="math inline">\(X\)</span>), and this becomes multiple linear regression. The visualization of this kind of data is more difficult, and for example purposes, we will only use one explanatory variable, <span class="math inline">\(X\)</span>.</p>
</div>
<div id="linear-assumptions" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Assumptions of Linear Models</h2>
<p>A linear model might very well be a good model if we have data like that shown in Figures <a href="intro.html#fig:linearscatterplot">2.1</a> and <a href="intro.html#fig:linearline">2.2</a>. However, there are many cases where it might be inappropriate to use a linear model. To understand these cases, we first review the assumptions of linear models.</p>
<p>Linear models assume:</p>
<ul>
<li>The relationship between the explanatory variables and the response is linear.</li>
<li>The samples are independent.</li>
<li>The errors are normally distributed with mean 0 and constant variance.</li>
</ul>
<p>You might have seen these assumptions written in notation as such.</p>
<p><span class="math display">\[y_i  = \beta_0 + \beta_1 x_i + \epsilon_i \]</span>
where <span class="math inline">\((x_i, y_i)\)</span> represents the observed values for observation <span class="math inline">\(i = 1, 2, \ldots, n\)</span> and
<span class="math display">\[ \epsilon_i \overset{\text{iid}}{\sim} N(0,\sigma^2)\]</span></p>
<p>In words, this means that the errors are independent and identically distributed from a normal distribution with mean 0 and constant variance <span class="math inline">\(\sigma^2\)</span> (Notice how there is no subscript <span class="math inline">\(i\)</span> for the variance. This means that all our our <span class="math inline">\(y_i\)</span> values will have the same variation, regardless of what their associated values of <span class="math inline">\(x_i\)</span> are). If we are performing multiple linear regression with up to <span class="math inline">\(p\)</span> explanatory variables then this first assumption becomes <span class="math inline">\(y_i = \beta_0 + \beta_1 x_{1,i} + \ldots + \beta_p x_{p,i} + \epsilon_i\)</span>. That is, our response variable, <span class="math inline">\(Y\)</span>, is a <em>linear function</em> of the explanatory variables <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>.</p>
</div>
<div id="reframing-linear-model" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Reframing Linear Models as Generalized Linear Models</h2>
<p>To see how the linear model is just a special type of Generalized Linear Model, we will slightly modify how we think of these assumptions, so as to be able to more easily see the parallels. Instead of the response <span class="math inline">\(y_i\)</span>, now consider the mean response <span class="math inline">\(\mu_i\)</span>, which for a given value of one predictor <span class="math inline">\(x_i\)</span> is the mean, or expected value of all responses with the value of that explanatory variable.</p>
<p>Then our assumptions become:</p>
<p><span class="math display" id="eq:systematic-linear">\[\begin{equation}
\mu_i = \beta_0 + \beta_1 x_i \tag{2.1}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:random-linear">\[\begin{equation}
y_i \overset{\text{iid}}{\sim} N(\mu_i, \sigma^2)\tag{2.2}
\end{equation}\]</span></p>
<p>We refer to the first condition, Equation <a href="intro.html#eq:systematic-linear">(2.1)</a>, as the <strong>Systematic Component</strong> and the second condition, Equation <a href="intro.html#eq:random-linear">(2.2)</a>, as the <strong>Random Component</strong>. In this framework, the systematic component describes how the explanatory variables impact some parameter, in this case the mean. Once we model how the mean parameter changes depending on the explanatory variable, the random component then describes the variability we see in the response given the value of the parameter.</p>
<p>Generalized Linear Models are inspired by this idea of generalizing the random and systematic component of linear models. More broadly, a Generalized Regression Model has a systematic component</p>
<p><span class="math display">\[ g(\mu_i) = \beta_0 + \beta_1 x_i + \epsilon_i\]</span></p>
<p>where, in order to generalize the systematic component, we use a link function <span class="math inline">\(g(y)\)</span> which then requires some function of the response to be linearly related to our explanatory variables.</p>
<p>The random component can then be written in general as:</p>
<p><span class="math display">\[y_i \overset{\text{iid}}{\sim} EDM(g(\mu_i), \phi) \]</span></p>
<p>Where EDM stands for a probability distribution from the Exponential Dispersion Family (EDM). We discuss these EDMs in greater detail in the next chapter but we can note for now that the Normal, Binomial, and Poisson distributions all belong to the Exponential Dispersion Family. The EDM distribution we use depends on the mean response, as well as a dispersion parameter <span class="math inline">\(\phi\)</span>. For many commonly use GLMs, and the ones discussed in this book, this <span class="math inline">\(\phi\)</span> is known.</p>
<p>We note that normal linear models fall easily into this framework, where <span class="math inline">\(g(y_i) = y_i\)</span> the identity function, and use the Normal distribution as our random component.</p>
<p>Deciding on what Random and Systematic component to use in a model requires ….
<!-- ZZQ: Here's my attempt at wrapping this section up -->
us to have some idea about both the possible values our response variable takes as well as having some idea about which probability distribution might have generated our response.</p>
</div>
<div id="what-happens-when-we-break-the-assumptions-of-linear-models" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> What happens when we break the assumptions of linear models</h2>
<p>How can we tell when these assumptions are violated? We can examine both the random and systematic component and see if our data can/should be modeled in such a way. This largely comes from knowledge of the data.</p>
<p>Linear models are generally robust, and can be reasonable when assumptions are not exactly met. However, if we know assumptions are not met, and how they are not met, it is appropriate to use a more appropriate model for the data.</p>
<!-- ZZQ: Maybe here we can create an example plot from a Bernoulli random variable. That way, we can (1) show that a linear model will still "fit" the data but (2) the residuals no long show that the assumptions are satisfied and (3) the output of the linear model no long makes sense (i.e. if our response is {0,1} then a model predicting some value less than 0 or greater than 1 doesn't make sense.) -->
</div>
<div id="parameter-estimation" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Parameter estimation</h2>
<!-- ZZQ: Should this section be placed in CHapter 3? -->
<p>Another difference between linear models and generalized linear models is the way we estimate the parameters <span class="math inline">\(\beta\)</span>. For linear models, we find the minimize the sums of squares from the predictor to the response(s). This has a closed form solution, and can be calculated by hand (if one <strong>really</strong> wanted to). For generalized linear models, we estimate the <span class="math inline">\(\beta\)</span> parameters using Maximum Likelihood Estimation. For the Normal linear model case, this is equivalent to minimizing the sums of squares. However, for other GLMs, there is no closed form solution, which requires us to perform an iterative algorithm to land at the parameter estimates. Luckily this is all done behind the scenes in R. Those interested in learning more about how parameter estimation works for GLMs can read Chapter 6 of <span class="citation">(Dunn and Smyth <a href="#ref-dunn2018generalized" role="doc-biblioref">2018</a>)</span>.</p>
</div>
<div id="lms-and-glms-in-r" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> LMs and GLMs in R</h2>
<p>Fitting GLMs in R is very similar to fitting linear models in R. For linear models, we use the function <code>lm()</code>. For generalized regression models, we use the function <code>glm()</code>. Both require a formula input, but <code>glm()</code> also requires the user to specify the random and systematic part of a GLM, by specifying the <code>family</code> and <code>link</code> function. Help for fitting a GLM can be found by <code>?glm()</code> and reading more about the family argument.</p>
<p>Let’s use some simulated data to see how GLMs are fit in R, and</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="intro.html#cb1-1"></a><span class="kw">head</span>(sim_data)</span></code></pre></div>
<pre><code>##           y     x1    x2
## 1 -773.3304 -66.25 213.2
## 2 -341.5198 246.50 277.8
## 3 -966.8550 -70.00 282.1
## 4 -330.2822 210.50 254.6
## 5 -610.0071 146.00 296.5
## 6 -718.5686  36.50 264.2</code></pre>
<p>First, fit a linear model the normal way, using <code>lm()</code></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="intro.html#cb3-1"></a>lm_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> sim_data)</span>
<span id="cb3-2"><a href="intro.html#cb3-2"></a><span class="kw">summary</span>(lm_mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = sim_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -58.754 -14.070   0.641  15.151  48.743 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -6.22134   24.20088  -0.257    0.798    
## x1           1.97638    0.02730  72.404   &lt;2e-16 ***
## x2          -2.96299    0.09424 -31.440   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.4 on 72 degrees of freedom
## Multiple R-squared:  0.9888, Adjusted R-squared:  0.9885 
## F-statistic:  3186 on 2 and 72 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Now we show that we can use <code>glm()</code> the same way, specifying <code>family = "gaussian"</code>, which gives the identity link.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="intro.html#cb5-1"></a>glm_mod &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> sim_data, <span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb5-2"><a href="intro.html#cb5-2"></a><span class="kw">summary</span>(glm_mod)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x1 + x2, family = &quot;gaussian&quot;, data = sim_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -58.754  -14.070    0.641   15.151   48.743  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -6.22134   24.20088  -0.257    0.798    
## x1           1.97638    0.02730  72.404   &lt;2e-16 ***
## x2          -2.96299    0.09424 -31.440   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 547.5034)
## 
##     Null deviance: 3528340  on 74  degrees of freedom
## Residual deviance:   39420  on 72  degrees of freedom
## AIC: 690.68
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>As you can see from the summary outputs, we get results which are very similar, as we should, since both functions fit the same model. However, some outputs of our linear model are slightly different when we compare them to the GLM version. For instance, we no longer have values for the residual standard errors, R-squared values and F-statistics. Instead, in the GLM, we have summary values like the null and residual deviance along with the AIC. We’ll talk more about these differences in future chapters. But for now, know that many of the functions we used for linear models, like <code>coef()</code>, <code>predict()</code>, etc, will perform as expected, when we pass a generalized linear model fitted using <code>glm()</code> in as the first argument.</p>
</div>
<div id="definitions" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Some definitions</h2>
<p>We close this chapter with some definitions of common terminology and explanation of notation that will be used throughout this text.</p>
<p>Predictor - the thing on the y-axis
Explanatory variable - the stuff on the x-axis. Note that we can have more than one (but won’t plot it then), and then this becomes multiple variable regression.</p>
<p>Something that is an estimated quantity will have a hat over it.
For example, we might assume that there is some ‘true’ (but unknown) linear relationship between our explanatory variables and our predictor.</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x\]</span></p>
<p>From our sample data, we use a linear model to make an estimate of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>,</p>
<p>so our estimate/best guess of this true model relationship is</p>
<p><span class="math display">\[ \hat y = \hat\beta_0 + \hat\beta_1 x\]</span>
We of course want our <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> to be a ‘good’ and ‘close’ estimate of the unknown quantities <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Ideas of what ‘good’ and ‘close’ mean will be covered in the next section.</p>
</div>
<div id="conclusion" class="section level2" number="2.8">
<h2><span class="header-section-number">2.8</span> Conclusion</h2>
<p>Linear models are not always the best tool for describing relationship in data. Luckily we can generalize the ideas and framework developed in linear models to hold for more general cases to create GLMs. Using a more general framework and more general assumptions allows us to build tools that will hold for all GRMs. The most notable of these that we will further explore are GRMs for binary data (ch4) and count data (ch5)</p>
</div>
<div id="examples" class="section level2" number="2.9">
<h2><span class="header-section-number">2.9</span> Examples</h2>
<p>Perhaps some examples of data and students can tell what type of data it should be modeled by?</p>
<p>Explore the following case of where a linear model is NOT appropriate, and then fit a <code>glm()</code> call in R using an appropriate family and link function.</p>
<p>Note that this example might take a minute to load.</p>
<iframe src="https://emilypalmer.shinyapps.io/GRM_LearnR/" width="672" height="800px">
</iframe>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-dunn2018generalized">
<p>Dunn, Peter K, and Gordon K Smyth. 2018. <em>Generalized Linear Models with Examples in R</em>. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="how-are-glms-different.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Bookdown.pdf", "Bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
