<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction | An Introduction to Generalized Linear Models</title>
  <meta name="description" content="Generalized Linear Models for non-statistics graduate students" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction | An Introduction to Generalized Linear Models" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Generalized Linear Models for non-statistics graduate students" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction | An Introduction to Generalized Linear Models" />
  
  <meta name="twitter:description" content="Generalized Linear Models for non-statistics graduate students" />
  

<meta name="author" content="Emma Grossman, Leah Marcus, Emily Palmer, Katherine Pulham, Andrew Rumments" />


<meta name="date" content="2021-03-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="how-are-glms-different.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">GRMs</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About this book</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#linear-models---a-kind-of-generalized-linear-model"><i class="fa fa-check"></i><b>2.1</b> Linear models - a kind of Generalized Linear Model</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#linear-assumptions"><i class="fa fa-check"></i><b>2.2</b> Assumptions of Linear Models</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-happens-when-we-break-the-assumptions-of-linear-models"><i class="fa fa-check"></i><b>2.3</b> What happens when we break the assumptions of linear models</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#parameter-estimation"><i class="fa fa-check"></i><b>2.4</b> Parameter estimation</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#lms-and-glms-in-r"><i class="fa fa-check"></i><b>2.5</b> LMs and GLMs in R</a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#definitions"><i class="fa fa-check"></i><b>2.6</b> Some definitions</a></li>
<li class="chapter" data-level="2.7" data-path="intro.html"><a href="intro.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
<li class="chapter" data-level="2.8" data-path="intro.html"><a href="intro.html#examples"><i class="fa fa-check"></i><b>2.8</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html"><i class="fa fa-check"></i><b>3</b> How are GLMs “different”?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#introdution"><i class="fa fa-check"></i><b>3.1</b> Introdution</a></li>
<li class="chapter" data-level="3.2" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#assumptions-of-a-glm"><i class="fa fa-check"></i><b>3.2</b> Assumptions of a GLM</a></li>
<li class="chapter" data-level="3.3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#framework"><i class="fa fa-check"></i><b>3.3</b> Framework</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#exponential-dispersion-models"><i class="fa fa-check"></i><b>3.3.1</b> Exponential Dispersion models</a></li>
<li class="chapter" data-level="3.3.2" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#properties-of-edms"><i class="fa fa-check"></i><b>3.3.2</b> Properties of EDMs</a></li>
<li class="chapter" data-level="3.3.3" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#linking-the-edm-to-the-explanatory-data"><i class="fa fa-check"></i><b>3.3.3</b> Linking the EDM to the explanatory data</a></li>
<li class="chapter" data-level="3.3.4" data-path="how-are-glms-different.html"><a href="how-are-glms-different.html#formal-definition-of-a-glm"><i class="fa fa-check"></i><b>3.3.4</b> Formal definition of a GLM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>4</b> Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear.html"><a href="linear.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="linear.html"><a href="linear.html#a-good-example-of-simple-linear-regression"><i class="fa fa-check"></i><b>4.2</b> A “Good” Example of Simple Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear.html"><a href="linear.html#a-bad-example-of-simple-linear-regression"><i class="fa fa-check"></i><b>4.3</b> A “Bad” Example of Simple Linear Regression</a></li>
<li class="chapter" data-level="4.4" data-path="linear.html"><a href="linear.html#summary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#what-is-binomial-data"><i class="fa fa-check"></i><b>5.1</b> What is Binomial Data?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#refresher-bernoulli-and-binomial"><i class="fa fa-check"></i><b>5.1.1</b> Refresher: Bernoulli and Binomial</a></li>
<li class="chapter" data-level="5.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#representing-the-bernoulli-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Representing the Bernoulli distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#representing-the-binomial-distribution"><i class="fa fa-check"></i><b>5.1.3</b> Representing the binomial distribution</a></li>
<li class="chapter" data-level="5.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#what-does-a-bunch-of-binomial-data-look-like-then"><i class="fa fa-check"></i><b>5.1.4</b> What does a bunch of binomial data look like then?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#why-doesnt-ols-work-for-binomial-data"><i class="fa fa-check"></i><b>5.2</b> Why doesn’t OLS work for Binomial Data?</a></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#link-functions-we-can-use"><i class="fa fa-check"></i><b>5.3</b> Link functions we can use</a></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#ed50"><i class="fa fa-check"></i><b>5.4</b> ED50</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i><b>6</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="poisson-regression.html"><a href="poisson-regression.html#what-is-poisson-data"><i class="fa fa-check"></i><b>6.1</b> What is Poisson data?</a></li>
<li class="chapter" data-level="6.2" data-path="poisson-regression.html"><a href="poisson-regression.html#why-ordinary-least-squares-does-not-work-for-poisson-data"><i class="fa fa-check"></i><b>6.2</b> Why ordinary least squares does not work for Poisson data</a></li>
<li class="chapter" data-level="6.3" data-path="poisson-regression.html"><a href="poisson-regression.html#link-functions-for-poisson-glms"><i class="fa fa-check"></i><b>6.3</b> Link functions for Poisson GLM’s</a></li>
<li class="chapter" data-level="6.4" data-path="poisson-regression.html"><a href="poisson-regression.html#poisson-example"><i class="fa fa-check"></i><b>6.4</b> Poisson Example</a></li>
<li class="chapter" data-level="6.5" data-path="poisson-regression.html"><a href="poisson-regression.html#problems-of-overdispersion-and-solutions"><i class="fa fa-check"></i><b>6.5</b> Problems of overdispersion and solutions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Generalized Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction</h1>
<div id="linear-models---a-kind-of-generalized-linear-model" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Linear models - a kind of Generalized Linear Model</h2>
<p>If you have fit a linear model before, congratulations! You have already fit (one case) of a generalized linear model (GLM). Over the course of this book, we will explore the framework of Generalized Linear Models, why the Linear Model is a special case of a GLM, and two common type of GLMs, including logistic regression (for binary/binomial count data) and poisson regression (for count data).</p>
<p>Let’s review why we would fit a linear model. For linear regression, given our data, (if we make some key assumptions <a href="intro.html#linear-assumptions">2.2</a> ), we can perform inference or prediction by assuming that our response value forms a linear relationship with our explanatory variable (or variables). Generalized linear models expand upon the linear model assumptions to increase the kinds and ways we can use data to answer questions.</p>
<p>The reasoning behind linear models can be intuitive; if we have two continuous variables, and make a scatterplot our data, and see this:</p>
<p><img src="Bookdown_files/figure-html/linearscatterplot-1.png" width="672" /></p>
<p>we might want to fit a straight line through the cloud of points, i.e. modeling the relationship linearly.</p>
<p><img src="images/put_a_bird.png" style="width:50.0%" /></p>
<p><img src="Bookdown_files/figure-html/linearline-1.png" width="672" /></p>
<p>To interpret this relationship and make predictions, we need to know the slope and intercept of this line. This is done by minimizing the least squares, which will be explored in Chapter 3 @ref{linear}. (will it? should it?) Linear models can also have multiple explanatory variables (<span class="math inline">\(X_1, \ldots X_n\)</span> instead of just one <span class="math inline">\(X\)</span>), and this becomes multiple linear regression. The visualization of this kind of data is more difficult, and for example purposes, we will only use one <span class="math inline">\(X\)</span>.</p>
</div>
<div id="linear-assumptions" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Assumptions of Linear Models</h2>
<p>A linear model might very well be a good model if we have data like the above. However, there are many cases where it might be inappropriate to use a linear model. To understand these cases, we first review the assumptions of linear models.</p>
<p>Linear models assume:</p>
<ul>
<li>The relationship between the explanatory variables and the response is linear.</li>
<li>The samples are independent.</li>
<li>The errors are normally distributed with mean 0 and constant variance.</li>
</ul>
<p>You might have seen these assumptions written in notation as such.</p>
<p><span class="math display">\[y_i  = \beta_0 + \beta_1 x + \epsilon_i \]</span>
where
<span class="math display">\[ \epsilon_i \sim \text{iid } N(0,\sigma^2)\]</span></p>
<p>In words, this means that the errors are independent and identically distributed by the normal distribution, with mean 0 and constant variance <span class="math inline">\(\sigma^2\)</span> (notice how there is no subscript <span class="math inline">\(i\)</span> for the variance). If are performing multiple linear regression, the first assumption becomes <span class="math inline">\(y_i = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p + \epsilon_i\)</span>.</p>
<p>To see how the linear model is a type of Generalized Linear Model, we will slightly modify how we think of these assumptions, so we will be able to more easily see the parallels. Instead of the response <span class="math inline">\(y_i\)</span>, now consider the mean response <span class="math inline">\(\mu_i\)</span>, which for a given value of one predictor <span class="math inline">\(x\)</span> is the mean, or expected value of all responses with the value of that explanatory variable.</p>
<p>Then our assumptions become:</p>
<p><span class="math display" id="eq:systematic-linear">\[\begin{equation}
\mu_i = \beta_0 + \beta_1 x \tag{2.1}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:random-linear">\[\begin{equation}
y_i \sim \text{ iid } N(\mu_i, \sigma^2)\tag{2.2}
\end{equation}\]</span></p>
<p>We refer to the first condition <a href="intro.html#eq:systematic-linear">(2.1)</a> as the <strong>Systematic Component</strong> and the second condition <a href="intro.html#eq:random-linear">(2.2)</a> as the <strong>Random Component</strong></p>
<p>Generalized Linear Models are inspired by generalizing the random and systematic component of linear models.</p>
<p>A Generalized Regression Model has a systematic component:</p>
<p><span class="math display">\[ g(\mu_i) = \beta_0 + \beta_1 x + \epsilon_i\]</span>
To generalized the systematic component, we use a link function <span class="math inline">\(g(y)\)</span>, so we now require some function of the response to be linearly related to our explanatory variables.</p>
<p>and a random component:</p>
<p><span class="math display">\[y_i \sim \text{ iid } EDM(g(\mu_i), \phi) \]</span>
In words,… to some probability distribution in the Exponential Dispersion Family (EDM), which will be discussed in the next chapter. Normal, Binomial, and Poisson distributions all belong to the Exponential Dispersion Family. The EDM distribution depends on the mean response, as well as a dispersion parameter <span class="math inline">\(\phi\)</span>. For many commonly use GLMs, and the ones discussed in this book, this <span class="math inline">\(\phi\)</span> is known.</p>
<p>We note that normal linear models fall easily into this framework, where <span class="math inline">\(g(y_i) = y_i\)</span> the identity function, and use the Normal distribution as our random component.</p>
<p>Deciding on what Random and Systematic component to use requires ….</p>
</div>
<div id="what-happens-when-we-break-the-assumptions-of-linear-models" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> What happens when we break the assumptions of linear models</h2>
<p>How can we tell when these assumptions are violated? We can examine both the random and systematic component and see if our data can/should be modeled in such a way. This largely comes from knowledge of the data.</p>
<p>Linear models are generally robust, and can be reasonable when assumptions are not exactly met. However, if we know assumptions are not met, and how they are not met, it is appropriate to use a more appropriate model for the data.</p>
</div>
<div id="parameter-estimation" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Parameter estimation</h2>
<p>Another difference between linear models and generalized linear models is the way we estimate the parameters <span class="math inline">\(\beta\)</span>. For linear models, we find the minimize the sums of squares from the predictor to the response(s). This has a closed form solution, and can be calculated by hand (if one <strong>really</strong> wanted to). For generalized linear models, we estimate the <span class="math inline">\(\beta\)</span> parameters using Maximum Likelihood Estimation. For the Normal linear model case, this is equivalent to minimizing the sums of squares. However, for other GLMs, there is no closed form solution, which requires us to perform an iterative algorithm to land at the parameter estimates. Luckily this is all done behind the scenes in R. Those interested in learning more about how parameter estimation works for GLMs can read Chapter 6 of <span class="citation">(Dunn and Smyth <a href="#ref-dunn2018generalized" role="doc-biblioref">2018</a>)</span>.</p>
</div>
<div id="lms-and-glms-in-r" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> LMs and GLMs in R</h2>
<p>Fitting GLMs in R is very similar to fitting linear models in R. For linear models, we use the function <code>lm()</code>. For generalized regression models, we use the function <code>glm()</code>. Both require a formula input, but <code>glm()</code> also requires the user to specify the random and systematic part of a GLM, by specifying the <code>family</code> and <code>link</code> function. Help for fitting a GLM can be found by <code>?glm()</code> and reading more about the family argument.</p>
<p>Let’s use some simulated data to see how GLMs are fit in R, and</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="intro.html#cb1-1"></a><span class="kw">head</span>(sim_data)</span></code></pre></div>
<pre><code>##           y     x1    x2
## 1 -411.7135 139.25 232.6
## 2 -777.7500 -88.00 203.9
## 3 -933.7452 -75.25 269.6
## 4 -688.6226 -26.50 217.5
## 5 -473.5069 201.50 288.0
## 6 -688.4904 -58.00 202.8</code></pre>
<p>First, fit a linear model the normal way, using <code>lm()</code></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="intro.html#cb3-1"></a>lm_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span><span class="st"> </span>x1<span class="op">+</span>x2, <span class="dt">data =</span> sim_data)</span>
<span id="cb3-2"><a href="intro.html#cb3-2"></a><span class="kw">summary</span>(lm_mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = sim_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -62.859 -14.235   1.878  14.103  62.505 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 45.58554   21.97880   2.074   0.0416 *  
## x1           2.01353    0.02967  67.875   &lt;2e-16 ***
## x2          -3.19035    0.08658 -36.847   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 24.11 on 72 degrees of freedom
## Multiple R-squared:  0.9873, Adjusted R-squared:  0.987 
## F-statistic:  2807 on 2 and 72 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Now we show that we can use <code>glm()</code> the same way, specifying <code>family = "gaussian"</code>, which gives the identity link.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="intro.html#cb5-1"></a>glm_mod &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span>x1<span class="op">+</span>x2, <span class="dt">data =</span> sim_data, <span class="dt">family =</span> gaussian)</span>
<span id="cb5-2"><a href="intro.html#cb5-2"></a><span class="kw">summary</span>(lm_mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = sim_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -62.859 -14.235   1.878  14.103  62.505 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 45.58554   21.97880   2.074   0.0416 *  
## x1           2.01353    0.02967  67.875   &lt;2e-16 ***
## x2          -3.19035    0.08658 -36.847   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 24.11 on 72 degrees of freedom
## Multiple R-squared:  0.9873, Adjusted R-squared:  0.987 
## F-statistic:  2807 on 2 and 72 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>So we get exactly the same output, as we should, since both functions fit the same model. Many functions used for linear models, like <code>coef()</code>, <code>predict()</code>, etc, will perform as expected, when we pass a generalized linear model fitted using <code>glm()</code> in as the first argument.</p>
</div>
<div id="definitions" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Some definitions</h2>
<p>We close this chapter with some definitions of common terminology and explanation of notation that will be used.</p>
<p>Predictor - the thing on the y-axis
Explanatory variable - the stuff on the x-axis. Note that we can have more than one (but won’t plot it then), and then this becomes multivariate regression.</p>
<p>Something that is an estimated quantity will have a hat over it.
For example, we might assume that there is some ‘true’ (but unknown) linear relationship between our explanatory variables and our predictor.</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x\]</span></p>
<p>From our sample data, we use a linear model to make an estimate of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>,</p>
<p>so our estimate/best guess of this true model relationship is</p>
<p><span class="math display">\[ \hat y = \hat\beta_0 + \hat\beta_1 x\]</span>
We of course want our <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> to be a ‘good’ and ‘close’ estimate of the unknown quantities <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Ideas of what ‘good’ and ‘close’ mean will be covered in the next section.</p>
</div>
<div id="conclusion" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Conclusion</h2>
<p>Linear models are not always the best tool for describing relationship in data. Luckily we can generalize the ideas and framework developed in linear models to hold for more general cases to create GLMs. Using a more general framework and more general assumptions allows us to build tools that will hold for all GRMs. The most notable of these that we will further explore are GRMs for binary data (ch4) and count data (ch5)</p>
</div>
<div id="examples" class="section level2" number="2.8">
<h2><span class="header-section-number">2.8</span> Examples</h2>
<p>Perhaps some examples of data and students can tell what type of data it should be modeled by?</p>
<p>Explore the following case of where a linear model is NOT appropriate, and then fit a <code>glm()</code> call in R using an appropriate family and link function.</p>
<p>Note that this example might take a minute to load.</p>
<iframe src="https://emilypalmer.shinyapps.io/GRM_LearnR/" width="672" height="800px">
</iframe>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-dunn2018generalized">
<p>Dunn, Peter K, and Gordon K Smyth. 2018. <em>Generalized Linear Models with Examples in R</em>. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="how-are-glms-different.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Bookdown.pdf", "Bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
